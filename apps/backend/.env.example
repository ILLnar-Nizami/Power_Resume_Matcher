# ===========================================
# LLM Provider Configuration
# ===========================================
# Supported providers: openai, anthropic, openrouter, gemini, deepseek, ollama, cerebras
LLM_PROVIDER=cerebras
LLM_MODEL=gpt-oss-120b
LLM_API_BASE=https://api.cerebras.ai

# For Ollama (local models)
# LLM_PROVIDER=ollama
# LLM_MODEL=gemma3:4b
# LLM_API_BASE=http://localhost:11434
#
# IMPORTANT: For Docker deployments with Ollama on host machine:
# Use host.docker.internal instead of localhost:
# LLM_API_BASE=http://host.docker.internal:11434
# (Works on macOS/Windows. On Linux, use your host IP or --network=host)

# For OpenRouter
# LLM_PROVIDER=openrouter
# LLM_MODEL=deepseek/deepseek-v3.2

# For Anthropic
# LLM_PROVIDER=anthropic
# LLM_MODEL=claude-haiku-4-5-20251001

# For Google Gemini
# LLM_PROVIDER=gemini
# LLM_MODEL=gemini/gemini-3-flash-preview

# For DeepSeek
# LLM_PROVIDER=deepseek
# LLM_MODEL=deepseek/deepseek-v3.2

# ===========================================
# Server Configuration
# ===========================================
HOST=0.0.0.0
PORT=8000

# Frontend URL - Used for PDF generation and CORS
FRONTEND_BASE_URL=http://localhost:3333

# CORS origins (comma-separated)
CORS_ORIGINS_LIST=http://localhost:3333,http://127.0.0.1:3333,http://localhost:8888,http://127.0.0.1:8888

# ===========================================
# Database Configuration
# ===========================================
# PostgreSQL connection string - REQUIRED
DATABASE_URL=postgresql+asyncpg://postgres:postgres_123@postgres:5432/resume_matcher

# Redis connection (optional)
REDIS_URL=redis://redis:6379/0

# Frontend URL - Used for PDF generation
FRONTEND_BASE_URL=http://localhost:3000

# ===========================================
# CORS Configuration
# ===========================================
CORS_ORIGINS=["http://localhost:3000","http://127.0.0.1:3000"]
